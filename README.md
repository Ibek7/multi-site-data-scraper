Multi-Site Data Scraper

A Python-based framework for scraping data from 50+ websites, tailored for research and analysis across diverse domains including academia, news, policy, and statistical platforms.

ğŸ“œ Overview

This repository is a comprehensive and modular web scraping framework designed to streamline data collection from a wide array of sources. Whether youâ€™re conducting research, monitoring news trends, or analyzing policies, this tool provides a robust foundation for automating your workflows.

Key Features:
	â€¢	ğŸ“° Multi-Site Coverage: Support for over 50 websites, with easy extension for more.
	â€¢	ğŸ¯ Modular Design: Each scraper is independently configured, making maintenance and updates efficient.
	â€¢	ğŸ§ª Unit Testing: Includes tests to ensure reliability and minimize disruptions from website changes.
	â€¢	ğŸ› ï¸ Customizable Configurations: YAML-based configuration for selectors, headers, and site-specific settings.
	â€¢	ğŸ“Š Focus on Research Needs: Optimized for data analysts, policy researchers, and academics.

ğŸš€ Getting Started

Prerequisites

Ensure the following dependencies are installed on your system:
	â€¢	Python 3.9+
	â€¢	pip (Python package manager)

Installation

Clone the repository and set up your environment:

git clone https://github.com/<username>/multi-site-data-scraper.git
cd multi-site-data-scraper
python -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
pip install -r requirements.txt

Configuration

Configure the scraper by editing the YAML files in the configs/ directory:
	â€¢	Example:
           africanews:
                      url: "https://www.africanews.com"
                      selectors:
                      article_link: ".teaser_title a"

Running the Scraper

Use the main script to scrape data:
                                  python main.py --config configs/news.yaml


ğŸ§ª Testing

Run the unit tests to validate functionality:

                                             python -m unittest discover tests/

ğŸ“‚ Repository Structure

multi-site-data-scraper/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ settings.yaml        # General scraper settings (e.g., headers, delays)
â”‚   â”œâ”€â”€ academic.yaml        # Configurations for academic scrapers
â”‚   â”œâ”€â”€ news.yaml            # Configurations for news scrapers
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Raw scraped data
â”‚   â”œâ”€â”€ processed/           # Cleaned and structured data
â”‚   â”œâ”€â”€ logs/                # Logs for debugging and monitoring scraper runs
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ README.md            # Project overview and setup instructions
â”‚   â”œâ”€â”€ CONTRIBUTING.md      # Guidelines for collaboration
â”‚   â”œâ”€â”€ scraper_guidelines.md # Instructions for creating new scrapers
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ data_cleaning.ipynb  # Jupyter notebook for preprocessing data
â”‚   â”œâ”€â”€ analysis.ipynb       # Jupyter notebook for data visualization and exploration
â”‚
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ academic/            # Academic scraper scripts
â”‚   â”‚   â”œâ”€â”€ pubmed_scraper.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ news/                # News scraper scripts
â”‚   â”œâ”€â”€ ngo_policy/          # NGO and policy organization scrapers
â”‚   â”œâ”€â”€ data_sources/        # Data and statistical sources scrapers
â”‚   â”œâ”€â”€ regional/            # Regional and specialized platform scrapers
â”‚   â”œâ”€â”€ aggregators/         # Aggregators and open-access site scrapers
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_scrapers.py     # Test scripts for individual scrapers
â”‚   â”œâ”€â”€ test_utils.py        # Test scripts for utilities
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ downloader.py        # Utility functions for downloading data
â”‚   â”œâ”€â”€ parser.py            # Utility functions for parsing data
â”‚   â”œâ”€â”€ logger.py            # Centralized logging utilities
â”‚   â”œâ”€â”€ database.py          # Data storage and database utilities (e.g., SQLite, MongoDB)
â”‚
â”œâ”€â”€ .gitignore               # Files and folders to ignore in Git
â”œâ”€â”€ requirements.txt         # Python dependencies for consistent environments
â”œâ”€â”€ Dockerfile               # Docker configuration for containerized environments
â”œâ”€â”€ main.py                  # Entry point for running all or selected scrapers

ğŸ“š Documentation

Detailed documentation is available in the docs/ folder, including:
	â€¢	Guidelines for adding new scrapers.
	â€¢	Examples of advanced use cases.
	â€¢	Common troubleshooting steps.

ğŸ› ï¸ Built With
	â€¢	BeautifulSoup - HTML parsing library
	â€¢	Requests - HTTP library for making web requests
	â€¢	PyYAML - For handling YAML configurations
	â€¢	Docker - For containerized deployments

ğŸŒŸ Contributing

Contributions are welcome! If youâ€™d like to add new features, improve the existing ones, or fix bugs:
	1.	Fork the repository.
	2.	Create a feature branch (git checkout -b feature-name).
	3.	Commit your changes (git commit -m "Add feature-name").
	4.	Push to the branch (git push origin feature-name).
	5.	Open a pull request.


ğŸ¤ Acknowledgments

Special thanks to Professor Henry Dambanemuya for their mentorship and guidance in making this project. Your insights into data analysis and web technologies have greatly influenced the design and functionality of this framework.

ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

ğŸ“§ Contact

For any inquiries or support, please feel free to reach out:
	â€¢	Name: Bekam Guta
	â€¢	Email: bekamdawit551@gmail.com
	â€¢	LinkedIn: http://linkedin.com/in/bekam-guta/
